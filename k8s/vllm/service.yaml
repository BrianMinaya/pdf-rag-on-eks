# =============================================================================
# vLLM SERVICE
# =============================================================================
# ClusterIP Service for the vLLM inference server.
# The chat-api calls this to generate answers: http://vllm:8000
#
# vLLM exposes an OpenAI-compatible API, so the chat-api uses it like:
#   POST http://vllm:8000/v1/chat/completions
#   {
#     "model": "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4",
#     "messages": [{"role": "user", "content": "..."}],
#     "temperature": 0.1,
#     "max_tokens": 1024
#   }
# =============================================================================

apiVersion: v1
kind: Service
metadata:
  name: vllm
  namespace: pdf-rag-chatbot
  labels:
    app: vllm
    project: pdf-rag-chatbot
spec:
  type: ClusterIP
  selector:
    app: vllm
  ports:
    - name: http
      port: 8000
      targetPort: 8000
      protocol: TCP
