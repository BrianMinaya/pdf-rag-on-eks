"""
Pydantic request/response models for the Chat API.

These models define the "contract" between the API and its clients. Pydantic
validates incoming data against these schemas automatically -- if a client
sends a request missing a required field or with the wrong type, FastAPI
returns a clear 422 error without our code needing to handle it.

This is especially useful for a RAG chatbot because:
- We need to clearly communicate what the API expects (question + optional history)
- We need to clearly communicate what the API returns (answer + sources with metadata)
- Source citations are a key part of RAG -- users need to verify the LLM's answer
  against the original documents, so we return detailed source information.
"""

from pydantic import BaseModel


class Source(BaseModel):
    """
    A source chunk that was used to generate the answer.

    In RAG (Retrieval-Augmented Generation), the LLM's answer is grounded in
    specific document chunks retrieved from the vector database. Each Source
    represents one of those chunks, so the user can:
    1. Verify the answer against the original text
    2. Navigate to the specific page in the PDF for more context
    3. Assess how relevant each source was (via the similarity score)
    """

    text: str
    """
    The actual chunk text that was retrieved from the vector database.

    This is a segment of the original PDF document (typically ~512 tokens,
    roughly 380 words) that the semantic search determined was relevant
    to the user's question.
    """

    page_number: int
    """
    The page number in the original PDF where this chunk came from.

    This is stored as metadata alongside the vector in Qdrant during
    ingestion, so we can trace every chunk back to its exact source location.
    """

    source: str
    """
    The filename of the PDF document this chunk came from.

    When multiple PDFs are ingested, this tells the user which specific
    document contained the relevant information.
    """

    score: float
    """
    The cosine similarity score between the question and this chunk (0 to 1).

    Cosine similarity measures how "close" two vectors are in direction
    (ignoring magnitude). In plain language:
    - 1.0 = Perfect match (the chunk is about exactly the same thing)
    - 0.8+ = Very relevant
    - 0.5-0.8 = Somewhat relevant
    - Below 0.5 = Probably not relevant

    We return this so users and developers can gauge how confident the
    system is in each retrieved source. If all scores are low, the answer
    may not be reliable.
    """


class ChatRequest(BaseModel):
    """
    Incoming chat request from the user.

    This is the payload that clients send to POST /chat. At minimum, they
    need to provide a question. Optionally, they can include conversation
    history to enable multi-turn conversations (follow-up questions).
    """

    question: str
    """The natural-language question the user is asking about the PDF documents."""

    session_id: str | None = None
    """
    Optional session identifier for future conversation tracking.

    This isn't used yet, but is included in the schema so that when we add
    server-side conversation persistence (storing chat history in PostgreSQL),
    clients won't need to change their request format.
    """

    history: list[dict] | None = None
    """
    Optional conversation history for multi-turn conversations.

    Format follows the OpenAI chat message format:
    [
        {"role": "user", "content": "What is the refund policy?"},
        {"role": "assistant", "content": "According to page 5..."},
        {"role": "user", "content": "Does that apply to digital products?"}
    ]

    Including history lets the LLM understand the *context* of follow-up
    questions. Without it, a question like "Does that apply to digital
    products?" would be meaningless because the LLM wouldn't know what
    "that" refers to.

    Currently, the client manages history. In the future, the server can
    store and retrieve it using the session_id.
    """


class ChatResponse(BaseModel):
    """
    Response returned to the user after the RAG pipeline processes their question.

    This contains the LLM-generated answer, the source chunks it was based on,
    and metadata about the generation. The sources are crucial for RAG -- they
    let users verify that the answer is actually grounded in the documents
    rather than being a hallucination (something the LLM made up).
    """

    answer: str
    """
    The LLM-generated answer to the user's question.

    This answer is generated by the LLM after being given the retrieved
    document chunks as context. It should include page number citations
    so users can verify the information.
    """

    sources: list[Source]
    """
    The document chunks that were retrieved and used as context for the answer.

    These are the top_k most similar chunks from the vector database.
    Returning them lets the frontend display the source text alongside
    the answer, giving users full transparency into what the LLM "saw."
    """

    model: str
    """
    The identifier of the LLM model that generated this answer.

    Useful for debugging and auditing -- if we switch models or test
    different versions, this tells us exactly which model produced each answer.
    """

    chunks_retrieved: int
    """
    How many chunks were retrieved from the vector database.

    This should usually equal the configured top_k value. If it's less,
    it might mean the collection doesn't have enough data, or the question
    is very unusual and few chunks matched.
    """
